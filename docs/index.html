<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DroneAudioSet - Augmented Human Lab</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>
<body>
    <header>
        <div class="header-content">
            <h1>DroneAudioSet</h1>
            <div class="authors">
                <span>Chitralekha Gupta<sup>*</sup>, Soundarya Ramesh<sup>*</sup></span>,
                <span>Praveen Sasikumar,Kian Peen Yeo,Suranga Nanayakkara</span>
                <div class="affiliations">
                    <sup>*</sup>equal contributors<br>
                    Augmented Human Lab, School of Computing, National University of Singapore
                </div>
            </div>
            <div class="conference">
                Submitted to Neurips Dataset and Benchmarks 2025
            </div>
        </div>
    </header>

    <nav>
        <ul>
            <li><a href="#abstract">Abstract</a></li>
            <li><a href="#downloads">Downloads</a></li>
            <li><a href="#citation">Citation</a></li>
            <li><a href="#contact">Contact</a></li>
        </ul>
    </nav>

    <main>
        <section id="abstract">
            <h2>Abstract</h2>
            <div class="content-box">
                <p>
                    Unmanned Aerial Vehicles (UAVs) or drones, are increasingly used in search and rescue missions to detect human presence. Existing systems primarily leverage vision-based methods which are prone to fail under low-visibility or occlusion. Drone-based audio perception offers promise but suffers from extreme ego-noise that masks sounds indicating human presence.  
Existing datasets are either limited in diversity or synthetic, lacking real acoustic interactions, and there are no standardized setups for drone audition. 
To this end, we present DroneAudioset (The dataset is publicly available at https://huggingface.co/datasets/ahlab-drone-project/DroneAudioSet/ under the MIT license), a comprehensive drone audition dataset featuring 23.5 hours of annotated recordings, covering a wide range of signal-to-noise ratios (SNRs) from -60 dB to 0 dB, across various drone types, throttles, microphone configurations as well as environments. The dataset enables development and systematic evaluation of noise suppression and classification methods for human-presence detection under challenging conditions, while also informing practical design considerations for drone audition systems, such as microphone placement trade-offs, and development of drone noise-aware audio processing. This dataset is an important step towards enabling design and deployment of drone-audition systems.
                </p>
                <div class="teaser-image">
                    <img src="images/data-collection-setup-v2.png" alt="Data Collection Setup">
                    <p class="caption">Figure (a) illustrates our experimental setup with the drone attached to a fixed aluminum frame, with two microphone arrays, \micup and \micdown, and a single microphone, \miccenter. The source sounds (i.e., human vocal sounds, human presence sounds and ambient sounds) are transmitted through a speaker. (b) the actual setup -- the drone frame, microphone array, and the drones used.</p>
                </div>
            </div>
        </section>

        <section id="downloads">
            <h2>Downloads</h2>
            <div class="content-box">
                <div class="download-item">
                    <i class="fas fa-file-pdf"></i>
                    <div>
                        <h3>Paper</h3>
                        <p><a href="#">Download PDF</a> (X MB)</p>
                    </div>
                </div>
                <div class="download-item">
                    <i class="fas fa-code"></i>
                    <div>
                        <h3>Code</h3>
                        <p><a href="https://github.com/augmented-human-lab/DroneAudioSet-code">GitHub Repository</a></p>
                    </div>
                </div>
                <div class="download-item">
                    <i class="fas fa-database"></i>
                    <div>
                        <h3>Dataset</h3>
                        <p><a href="https://huggingface.co/datasets/ahlab-drone-project/DroneAudioSet">Download DroneAudioset</a> ( GB)</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="citation">
            <h2>Citation</h2>
            <div class="content-box citation-box">
                <pre>
@inproceedings{author2023droneaudioset,
    title={DroneAudioSet: A Dataset for Audio-Based Drone Analysis},
    author={Author1, A. and Author2, B.},
    booktitle={Proceedings of the Conference},
    year={2023}
}</pre>
                <button onclick="copyCitation()">Copy Citation</button>
            </div>
        </section>

        <section id="contact">
            <h2>Contact</h2>
            <div class="content-box">
                <p>For questions and comments, please contact:</p>
                <p><i class="fas fa-envelope"></i> your.email@institution.edu</p>
            </div>
        </section>
    </main>

    <footer>
        <p>Â© 2023 Augmented Human Lab. Last updated: Month Day, Year.</p>
    </footer>

    <script>
        function copyCitation() {
            const citation = document.querySelector('.citation-box pre');
            navigator.clipboard.writeText(citation.innerText);
            alert('Citation copied to clipboard!');
        }
    </script>
</body>
</html>